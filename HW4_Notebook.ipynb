{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16d0905-696e-4008-a969-7ea332fc9e49",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q1 Load the json file into a dataframe. Display the dataframe and do a dataprofile on it."
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"dbfs:/FileStore/HW4-1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989504d5-c454-4af2-bae0-ecccde6fe84f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q1 Importing the libraries"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e387a7f-24ac-43d8-a4f0-cad8da5c6d96",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q1 Creating Spark Session"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark Session with Delta Lake\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"DeltaLakeHomework\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.0.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a669c6dd-edfb-4834-836c-b9c98f76f43b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q1 Load Json File into DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = spark.read \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d283491f-d59f-4731-996d-b72f2a777b95",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q1 Showing of DataFrame"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------+\n|Age|     Department|    Name|\n+---+---------------+--------+\n| 34|        Manager|  Alice1|\n| 30|        Analyst|  David1|\n| 28|Senior Engineer|    Bob1|\n| 29|         Intern|    Eve1|\n| 45|             HR|  Cathy1|\n| 31|        Manager|   John1|\n| 32|Senior Engineer|   Emma1|\n| 40|        Analyst|Michael1|\n| 27|         Intern| Sophia1|\n| 35|             HR| Daniel1|\n+---+---------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bfd1015-4906-460f-a8f2-beb5b887746e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q1 Describe Data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDataFrame Summary Statistics:\n+-------+-----------------+---------------+-------+\n|summary|              Age|     Department|   Name|\n+-------+-----------------+---------------+-------+\n|  count|               10|             10|     10|\n|   mean|             33.1|           null|   null|\n| stddev|5.665686189686118|           null|   null|\n|    min|               27|        Analyst| Alice1|\n|    max|               45|Senior Engineer|Sophia1|\n+-------+-----------------+---------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataFrame Summary Statistics:\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c76d8c47-5bd5-45a1-9448-a1485836e86e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q1 DataProfiling"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nPandas Profiling:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10 entries, 0 to 9\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Age         10 non-null     int64 \n 1   Department  10 non-null     object\n 2   Name        10 non-null     object\ndtypes: int64(1), object(2)\nmemory usage: 368.0+ bytes\nNone\n\nValue Counts for Department:\nManager            2\nAnalyst            2\nSenior Engineer    2\nIntern             2\nHR                 2\nName: Department, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Additional Profiling using Pandas\n",
    "pandas_df = df.toPandas()\n",
    "print(\"\\nPandas Profiling:\")\n",
    "print(pandas_df.info())\n",
    "print(\"\\nValue Counts for Department:\")\n",
    "print(pandas_df['Department'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7612d0-5b33-421a-ac1f-7ecc870c1a91",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q2 Save the dataframe as a delta table."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table saved at: /FileStore/delta_tables/employee_table\n"
     ]
    }
   ],
   "source": [
    "# Try direct saving without saveAsTable\n",
    "delta_path = \"/FileStore/delta_tables/employee_table\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "dbutils.fs.mkdirs(\"/FileStore/delta_tables\")\n",
    "\n",
    "# Save Delta table\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(f\"Delta table saved at: {delta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e764544e-57d2-4e4e-800d-8245523f032e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Question 3: reload the save table and the some data into your table. Using the versionof API, read the current and the prior version of the table and display them."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Version:\n+---+---------------+--------+\n|Age|     Department|    Name|\n+---+---------------+--------+\n| 34|        Manager|  Alice1|\n| 30|        Analyst|  David1|\n| 28|Senior Engineer|    Bob1|\n| 29|         Intern|    Eve1|\n| 45|             HR|  Cathy1|\n| 31|        Manager|   John1|\n| 32|Senior Engineer|   Emma1|\n| 40|        Analyst|Michael1|\n| 27|         Intern| Sophia1|\n| 35|             HR| Daniel1|\n+---+---------------+--------+\n\n\nUpdated Version:\n+---+---------------+--------+\n|Age|     Department|    Name|\n+---+---------------+--------+\n| 34|        Manager|  Alice1|\n| 30|        Analyst|  David1|\n| 28|Senior Engineer|    Bob1|\n| 29|         Intern|    Eve1|\n| 45|             HR|  Cathy1|\n| 31|        Manager|   John1|\n| 32|Senior Engineer|   Emma1|\n| 40|        Analyst|Michael1|\n| 27|         Intern| Sophia1|\n| 35|             HR| Daniel1|\n| 33|      Marketing|  Sarah1|\n+---+---------------+--------+\n\n\nTable Version History:\n+-------+-------------------+---------------+--------------------+-----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|         userId|            userName|        operation| operationParameters| job|         notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+---------------+--------------------+-----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|     14|2024-12-14 04:39:17|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1214-034253-tpnn7g8k|         13|WriteSerializable|        false|{numFiles -> 2, n...|        null|Databricks-Runtim...|\n|     13|2024-12-14 04:38:29|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1214-034253-tpnn7g8k|         12|WriteSerializable|        false|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|     12|2024-12-14 00:55:01|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|         11|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|     11|2024-12-14 00:54:57|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|         10|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|     10|2024-12-14 00:48:20|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|          9|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|      9|2024-12-14 00:48:17|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|          8|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|      8|2024-12-14 00:46:25|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|          7|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|      7|2024-12-14 00:46:21|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|          6|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|      6|2024-12-14 00:42:42|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|          5|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|      5|2024-12-14 00:42:38|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|          4|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|      4|2024-12-14 00:29:17|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          3|WriteSerializable|        false|{numFiles -> 9, n...|        null|Databricks-Runtim...|\n|      3|2024-12-14 00:29:07|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          2|WriteSerializable|        false|{numFiles -> 8, n...|        null|Databricks-Runtim...|\n|      2|2024-12-13 23:51:31|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          1|WriteSerializable|        false|{numFiles -> 9, n...|        null|Databricks-Runtim...|\n|      1|2024-12-13 23:51:18|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          0|WriteSerializable|        false|{numFiles -> 8, n...|        null|Databricks-Runtim...|\n|      0|2024-12-13 23:49:32|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|       null|WriteSerializable|        false|{numFiles -> 8, n...|        null|Databricks-Runtim...|\n+-------+-------------------+---------------+--------------------+-----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\nCurrent Version Data:\n+---+---------------+--------+\n|Age|     Department|    Name|\n+---+---------------+--------+\n| 34|        Manager|  Alice1|\n| 30|        Analyst|  David1|\n| 28|Senior Engineer|    Bob1|\n| 29|         Intern|    Eve1|\n| 45|             HR|  Cathy1|\n| 31|        Manager|   John1|\n| 32|Senior Engineer|   Emma1|\n| 40|        Analyst|Michael1|\n| 27|         Intern| Sophia1|\n| 35|             HR| Daniel1|\n| 33|      Marketing|  Sarah1|\n+---+---------------+--------+\n\nPrior Version Data:\n+---+---------------+--------+\n|Age|     Department|    Name|\n+---+---------------+--------+\n| 34|        Manager|  Alice1|\n| 30|        Analyst|  David1|\n| 28|Senior Engineer|    Bob1|\n| 29|         Intern|    Eve1|\n| 45|             HR|  Cathy1|\n| 31|        Manager|   John1|\n| 32|Senior Engineer|   Emma1|\n| 40|        Analyst|Michael1|\n| 27|         Intern| Sophia1|\n| 35|             HR| Daniel1|\n+---+---------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Function to update DataFrame\n",
    "def update_dataframe(spark, delta_path):\n",
    "    # Add a new record\n",
    "    new_data = [{\"Name\": \"Sarah1\", \"Age\": 33, \"Department\": \"Marketing\"}]\n",
    "    new_df = spark.createDataFrame(new_data)\n",
    "    \n",
    "    # Union with existing data\n",
    "    full_df = df.union(new_df)\n",
    "    \n",
    "    # Save as Delta table\n",
    "    full_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Create DeltaTable object\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Show initial version\n",
    "print(\"Initial Version:\")\n",
    "delta_table.toDF().show()\n",
    "\n",
    "# Update DataFrame to create a new version\n",
    "update_dataframe(spark, delta_path)\n",
    "\n",
    "# Refresh DeltaTable object\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Show updated version\n",
    "print(\"\\nUpdated Version:\")\n",
    "delta_table.toDF().show()\n",
    "\n",
    "# Display version history\n",
    "print(\"\\nTable Version History:\")\n",
    "delta_table.history().show()\n",
    "\n",
    "# Get the current version number from the history\n",
    "history = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_path}`\")\n",
    "current_version = history.select(\"version\").collect()[0][0]\n",
    "\n",
    "# Read the current version of the table\n",
    "current_version_df = spark.read.format(\"delta\").option(\"versionAsOf\", current_version).load(delta_path)\n",
    "print(\"Current Version Data:\")\n",
    "current_version_df.show()\n",
    "\n",
    "# Read the prior version of the table\n",
    "prior_version_df = spark.read.format(\"delta\").option(\"versionAsOf\", current_version - 1).load(delta_path)\n",
    "print(\"Prior Version Data:\")\n",
    "prior_version_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4daf6211-bffc-414a-9861-13b042f0afc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q3 Showing Updated Version"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUpdated Version:\n+---+---------------+--------+\n|Age|     Department|    Name|\n+---+---------------+--------+\n| 34|        Manager|  Alice1|\n| 30|        Analyst|  David1|\n| 28|Senior Engineer|    Bob1|\n| 29|         Intern|    Eve1|\n| 45|             HR|  Cathy1|\n| 31|        Manager|   John1|\n| 32|Senior Engineer|   Emma1|\n| 40|        Analyst|Michael1|\n| 27|         Intern| Sophia1|\n| 35|             HR| Daniel1|\n| 33|      Marketing|  Sarah1|\n+---+---------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Show updated version\n",
    "print(\"\\nUpdated Version:\")\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8a0796-5e93-46d2-8863-1ef8c1e68f2d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q4 Read the different versions using the option(readChangeData)...etc method. Reflect on why this did not work."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change Data:\n\nError with readChangeFeed: Error getting change data for range [0 , 14] as change data was not\nrecorded for version [0]. If you've enabled change data feed on this table,\nuse `DESCRIBE HISTORY` to see when it was first enabled.\nOtherwise, to start recording change data, use `ALTER TABLE table_name SET TBLPROPERTIES\n(delta.enableChangeDataFeed=true)`.\nNote: Change feed needs to be explicitly enabled.\n"
     ]
    }
   ],
   "source": [
    "# Attempt to read change data using readChangeFeed\n",
    "try:\n",
    "    # Start from the most recent version\n",
    "    recent_version_df = spark.read.format(\"delta\") \\\n",
    "        .option(\"readChangeFeed\", \"true\") \\\n",
    "        .option(\"startingVersion\", 0)  # Provide the starting version number\n",
    "    recent_version_df = recent_version_df.load(delta_path)\n",
    "    \n",
    "    # Show change data\n",
    "    print(\"Change Data:\")\n",
    "    recent_version_df.show()\n",
    "except Exception as e:\n",
    "    print(\"\\nError with readChangeFeed:\", str(e))\n",
    "    print(\"Note: Change feed needs to be explicitly enabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a3c2b19-e5f4-41c3-9438-8fce0a2e2135",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q4 Reflection Answer why Previous Command did not work"
    }
   },
   "outputs": [],
   "source": [
    "# CDF Not Enabled Initially:\n",
    "\n",
    "# Change Data Feed (CDF) was not enabled from the very first version (version 0) of the Delta table. Therefore, attempting to read changes starting from version 0 will fail because no change data was recorded for that version.\n",
    "# CDF Needs Explicit Enabling:\n",
    "\n",
    "# CDF needs to be explicitly enabled on the Delta table using the ALTER TABLE command. If it was enabled at a later version, you can only read change data from that version onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "199e7636-afda-4639-8f90-146aeed42193",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Q5: Checking Data Feed History"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------------+--------------------+-----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|         userId|            userName|        operation| operationParameters| job|         notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+---------------+--------------------+-----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|     14|2024-12-14 04:39:17|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1214-034253-tpnn7g8k|         13|WriteSerializable|        false|{numFiles -> 2, n...|        null|Databricks-Runtim...|\n|     13|2024-12-14 04:38:29|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1214-034253-tpnn7g8k|         12|WriteSerializable|        false|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|     12|2024-12-14 00:55:01|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|         11|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|     11|2024-12-14 00:54:57|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|         10|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|     10|2024-12-14 00:48:20|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|          9|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|      9|2024-12-14 00:48:17|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|          8|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|      8|2024-12-14 00:46:25|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|          7|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|      7|2024-12-14 00:46:21|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|          6|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|      6|2024-12-14 00:42:42|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|          5|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|      5|2024-12-14 00:42:38|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|          4|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|      4|2024-12-14 00:29:17|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          3|WriteSerializable|        false|{numFiles -> 9, n...|        null|Databricks-Runtim...|\n|      3|2024-12-14 00:29:07|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          2|WriteSerializable|        false|{numFiles -> 8, n...|        null|Databricks-Runtim...|\n|      2|2024-12-13 23:51:31|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          1|WriteSerializable|        false|{numFiles -> 9, n...|        null|Databricks-Runtim...|\n|      1|2024-12-13 23:51:18|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          0|WriteSerializable|        false|{numFiles -> 8, n...|        null|Databricks-Runtim...|\n|      0|2024-12-13 23:49:32|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|       null|WriteSerializable|        false|{numFiles -> 8, n...|        null|Databricks-Runtim...|\n+-------+-------------------+---------------+--------------------+-----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Check Change Data Feed history\n",
    "history = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_path}`\")\n",
    "history.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b54281f4-ac88-4925-9638-c0827be5cd49",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "# Q5 Enable CDC on the same table, do some changes, and read the change data between # the last two versions? Change the configuration of the api to read between version 0 and any of the # other versions? Reflect on why it worked/did not."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------------+--------------------+-----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|         userId|            userName|        operation| operationParameters| job|         notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+---------------+--------------------+-----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|     20|2024-12-14 04:51:37|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1214-034253-tpnn7g8k|         19|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|     19|2024-12-14 04:51:32|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1214-034253-tpnn7g8k|         18|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|     18|2024-12-14 04:48:20|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1214-034253-tpnn7g8k|         17|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|     17|2024-12-14 04:48:16|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1214-034253-tpnn7g8k|         16|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|     16|2024-12-14 04:47:20|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1214-034253-tpnn7g8k|         15|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|     15|2024-12-14 04:47:15|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1214-034253-tpnn7g8k|         14|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|     14|2024-12-14 04:39:17|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1214-034253-tpnn7g8k|         13|WriteSerializable|        false|{numFiles -> 2, n...|        null|Databricks-Runtim...|\n|     13|2024-12-14 04:38:29|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1214-034253-tpnn7g8k|         12|WriteSerializable|        false|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|     12|2024-12-14 00:55:01|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|         11|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|     11|2024-12-14 00:54:57|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|         10|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|     10|2024-12-14 00:48:20|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|          9|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|      9|2024-12-14 00:48:17|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|          8|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|      8|2024-12-14 00:46:25|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|          7|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|      7|2024-12-14 00:46:21|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|          6|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|      6|2024-12-14 00:42:42|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Append, ...|null|{151653019208167}|1213-233020-xh142jxx|          5|WriteSerializable|         true|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|      5|2024-12-14 00:42:38|616905300709848|surbhisharma9099@...|SET TBLPROPERTIES|{properties -> {\"...|null|{151653019208167}|1213-233020-xh142jxx|          4|WriteSerializable|         true|                  {}|        null|Databricks-Runtim...|\n|      4|2024-12-14 00:29:17|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          3|WriteSerializable|        false|{numFiles -> 9, n...|        null|Databricks-Runtim...|\n|      3|2024-12-14 00:29:07|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          2|WriteSerializable|        false|{numFiles -> 8, n...|        null|Databricks-Runtim...|\n|      2|2024-12-13 23:51:31|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          1|WriteSerializable|        false|{numFiles -> 9, n...|        null|Databricks-Runtim...|\n|      1|2024-12-13 23:51:18|616905300709848|surbhisharma9099@...|            WRITE|{mode -> Overwrit...|null|{151653019208167}|1213-233020-xh142jxx|          0|WriteSerializable|        false|{numFiles -> 8, n...|        null|Databricks-Runtim...|\n+-------+-------------------+---------------+--------------------+-----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\nonly showing top 20 rows\n\n\nChange Data between Versions:\n+---+----------+----+------------+---------------+-------------------+\n|Age|Department|Name|_change_type|_commit_version|  _commit_timestamp|\n+---+----------+----+------------+---------------+-------------------+\n| 36|     Sales|Tom1|      insert|             18|2024-12-14 04:48:20|\n+---+----------+----+------------+---------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Q5 Enable CDC on the same table, do some changes, and read the change data between\n",
    "# the last two versions? Change the configuration of the api to read between version 0 and any of the\n",
    "# other versions? Reflect on why it worked/did not.\n",
    "\n",
    "# Enable Change Data Feed\n",
    "spark.sql(\n",
    "    f\"ALTER TABLE delta.`{delta_path}` SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    ")\n",
    "\n",
    "# Make additional changes (you can modify this to match your use case)\n",
    "def make_more_changes(spark):\n",
    "    more_data = [{\"Name\": \"Tom1\", \"Age\": 36, \"Department\": \"Sales\"}]\n",
    "    more_df = spark.createDataFrame(more_data)\n",
    "    more_df.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "\n",
    "\n",
    "# Create additional version\n",
    "make_more_changes(spark)\n",
    "\n",
    "# Check Change Data Feed history\n",
    "history = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_path}`\")\n",
    "history.show()\n",
    "\n",
    "# Read CDC between last two versions\n",
    "print(\"\\nChange Data between Versions:\")\n",
    "# Adjust startingVersion to the version when CDF was first enabled\n",
    "changes = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"readChangeFeed\", \"true\")\n",
    "    .option(\"startingVersion\", 17)  # Adjust starting version based on history\n",
    "    .option(\"endingVersion\", 18)  # Adjust ending version based on history\n",
    "    .load(delta_path)\n",
    ")\n",
    "changes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43964492-1891-4265-b196-35610117364f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Did Not Work When Version Was Changed From 0 As ReadChangeFeed Was not enabled at that point of time"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nChange Data between Versions:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-204473638056691>:9\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mChange Data between Versions:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m changes \u001B[38;5;241m=\u001B[39m (spark\u001B[38;5;241m.\u001B[39mread\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreadChangeFeed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstartingVersion\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# Starting from version 0\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mendingVersion\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m10\u001B[39m)   \u001B[38;5;66;03m# Adjust ending version based on history\u001B[39;00m\n",
       "\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39mload(delta_path))\n",
       "\u001B[0;32m----> 9\u001B[0m changes\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m    917\u001B[0m     )\n",
       "\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n",
       "\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Error getting change data for range [0 , 10] as change data was not\n",
       "recorded for version [0]. If you've enabled change data feed on this table,\n",
       "use `DESCRIBE HISTORY` to see when it was first enabled.\n",
       "Otherwise, to start recording change data, use `ALTER TABLE table_name SET TBLPROPERTIES\n",
       "(delta.enableChangeDataFeed=true)`."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-204473638056691>:9\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mChange Data between Versions:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m changes \u001B[38;5;241m=\u001B[39m (spark\u001B[38;5;241m.\u001B[39mread\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreadChangeFeed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstartingVersion\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# Starting from version 0\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mendingVersion\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m10\u001B[39m)   \u001B[38;5;66;03m# Adjust ending version based on history\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39mload(delta_path))\n\u001B[0;32m----> 9\u001B[0m changes\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    917\u001B[0m     )\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Error getting change data for range [0 , 10] as change data was not\nrecorded for version [0]. If you've enabled change data feed on this table,\nuse `DESCRIBE HISTORY` to see when it was first enabled.\nOtherwise, to start recording change data, use `ALTER TABLE table_name SET TBLPROPERTIES\n(delta.enableChangeDataFeed=true)`.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Error getting change data for range [0 , 10] as change data was not\nrecorded for version [0]. If you've enabled change data feed on this table,\nuse `DESCRIBE HISTORY` to see when it was first enabled.\nOtherwise, to start recording change data, use `ALTER TABLE table_name SET TBLPROPERTIES\n(delta.enableChangeDataFeed=true)`.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read CDC between version 0 and another version\n",
    "print(\"\\nChange Data between Versions:\")\n",
    "changes = (spark.read\n",
    "    .format(\"delta\")\n",
    "    .option(\"readChangeFeed\", \"true\")\n",
    "    .option(\"startingVersion\", 0)  # Starting from version 0\n",
    "    .option(\"endingVersion\", 10)   # Adjust ending version based on history\n",
    "    .load(delta_path))\n",
    "changes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b126d858-b8fa-4f93-b250-5100eed94595",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reflection on Why It Worked/Did Not Work Why It Worked:  If the Change Data Feed (CDF) was enabled from the very beginning (version 0), then reading changes from version 0 to any other version should work without issues. This is because all changes would have been recorded from the start. Why It Did Not Work:  If the CDF was not enabled from the beginning, then attempting to read changes starting"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explanation\n",
    "# Reflection on Why It Worked/Did Not Work\n",
    "# Why It Worked:\n",
    "\n",
    "# If the Change Data Feed (CDF) was enabled from the very beginning (version 0), then reading changes from version 0 to any other version should work without issues. This is because all changes would have been recorded from the start.\n",
    "# Why It Did Not Work:\n",
    "\n",
    "# If the CDF was not enabled from the beginning, then attempting to read changes starting from version 0 will fail. This is because the changes were not recorded for the versions before CDF was enabled. In the provided history, CDF was enabled at version 9, so trying to read changes from version 0 will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2281e935-af9c-42bf-80b3-907f311c95fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HW4_Notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
